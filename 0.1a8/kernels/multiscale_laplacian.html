

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Multiscale Laplacian Kernel &mdash; GraKeL 0.1.8 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/_static/css/supplementary.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neighborhood Hash Kernel" href="neighborhood_hash.html" />
    <link rel="prev" title="Lovasz Theta Kernel" href="lovasz_theta.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> GraKeL
          

          
            
            <img src="../_static/logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../documentation.html">Documentation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">GraKeL</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../kernels.html">Kernels (between graphs)</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="core_framework.html">Core Kernel Framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="edge_histogram.html">Edge Histogram Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="graph_hopper.html">Graph Hopper Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="graphlet_sampling.html">Graphlet Sampling Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="hadamard_code.html">Hadamard Code Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="kernel.html">Kernel (general class)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lovasz_theta.html">Lovasz Theta Kernel</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Multiscale Laplacian Kernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#low-rank-approximation">Low Rank Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bibliography">Bibliography</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="neighborhood_hash.html">Neighborhood Hash Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="neighborhood_subgraph_pairwise_distance.html">Neighborhood Subgraph Pairwise Distance Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="odd_sth.html">ODD-STh Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="propagation.html">Propagation Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyramid_match.html">Pyramid Match Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="random_walk.html">Random Walk Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="shortest_path.html">Shortest Path Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="subgraph_matching.html">Subgraph Matching Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="svm_theta.html">SVM Theta Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="vertex_histogram.html">Vertex Histogram Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="weisfeiler_lehman.html">Weisfeiler Lehman Framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="weisfeiler_lehman_optimal_assignment.html">Weisfeiler-Lehman Optimal Assignment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../graph_kernel.html">GraphKernel (class)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graph.html">Graph (class)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../datasets.html">Dataset loading utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../classes.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">GraKeL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../api.html">GraKeL</a> &raquo;</li>
        
          <li><a href="../kernels.html">Kernels (between graphs)</a> &raquo;</li>
        
      <li>Multiscale Laplacian Kernel</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/kernels/multiscale_laplacian.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="multiscale-laplacian-kernel">
<span id="multiscale-laplacian"></span><h1>Multiscale Laplacian Kernel<a class="headerlink" href="#multiscale-laplacian-kernel" title="Permalink to this headline">Â¶</a></h1>
<p>The multiscale Laplacian graph kernel can handle unlabeled graphs, graphs with discrete node labels and graphs with continuous node attributes <a class="bibtex reference internal" href="#kondor2016multiscale" id="id1">[KP16]</a>.
It takes into account structure in graphs at a range of different scales by building a hierarchy of nested subgraphs.
These subgraphs are compared to each other using another graph kernel, called the feature space laplacian graph kernel.
This kernel is capable of lifting a base kernel defined on the vertices of two graphs to a kernel between the graphs themselves.
Since exact computation of the multiscale laplacian graph kernel is a very expensive operation, the kernel uses a randomized projection procedure  similar to the popular Nystr{&quot;o}m approximation for kernel matrices <a class="bibtex reference internal" href="#williams2001using" id="id2">[WS01]</a>.</p>
<p>Let <span class="math notranslate nohighlight">\(G=(V,E)\)</span> be an undirected graph such that <span class="math notranslate nohighlight">\(n = |V|\)</span>.
The Laplacian of <span class="math notranslate nohighlight">\(G\)</span> is a <span class="math notranslate nohighlight">\(n \times n\)</span> matrix defined as</p>
<div class="math notranslate nohighlight">
\[\mathbf{L} = \mathbf{D} - \mathbf{A}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the adjacency matrix of <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is a diagonal matrix such that <span class="math notranslate nohighlight">\(\mathbf{D}_{ii} = \sum_j \mathbf{A}_{ij}\)</span>.</p>
<p>Given two graphs <span class="math notranslate nohighlight">\(G_1\)</span> and <span class="math notranslate nohighlight">\(G_2\)</span> of <span class="math notranslate nohighlight">\(n\)</span> vertices, we can define the kernel between them to be a kernel between the corresponding normal distributions <span class="math notranslate nohighlight">\(p_1 = \mathcal{N}(\mathbf{0}, \mathbf{L_1}^{-1})\)</span> and <span class="math notranslate nohighlight">\(p_2 = \mathcal{N}(\mathbf{0}, \mathbf{L_2}^{-1})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-dimensional all-zeros vector.
More specifically, given two graphs <span class="math notranslate nohighlight">\(G_1\)</span> and <span class="math notranslate nohighlight">\(G_2\)</span> of <span class="math notranslate nohighlight">\(n\)</span> vertices with Laplacians <span class="math notranslate nohighlight">\(\mathbf{L_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{L_2}\)</span> respectively, the Laplacian graph kernel with parameter <span class="math notranslate nohighlight">\(\gamma\)</span> between the two graphs is</p>
<div class="math notranslate nohighlight">
\[k_{LG}(G_1, G_2) = \frac{| (\frac{1}{2} \mathbf{S}_1^{-1} + \frac{1}{2} \mathbf{S}_2^{-1} )^{-1} |^{1/2}}{|\mathbf{S}_1|^{1/4} |\mathbf{S}_2|^{1/4}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{S}_1 = \mathbf{L}_1^{-1} + \gamma \mathbf{I}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{S}_2 = \mathbf{L}_2^{-1} + \gamma \mathbf{I}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the <span class="math notranslate nohighlight">\(n \times n\)</span> identity matrix.
The Laplacian graph kernel captures similarity between the overall shapes of the two graphs.
However, it assumes that both graphs have the same size, and it is not invariant to permutations of the vertices.</p>
<p>To achieve permutation invariance, the multiscale Laplacian graph kernel represents each vertex as an <span class="math notranslate nohighlight">\(m\)</span>-dimensional vector whose components correspond to local and permutation invariant vertex features.
Such features may include for instance the degree of the vertex or the number of triangles in which it participates.
Then, it performs a linear transformation and represents each graph as a distribution of the considered features instead of a distribution of its vertices.
Let <span class="math notranslate nohighlight">\(\mathbf{U}_1, \mathbf{U}_2 \in \mathbb{R}^{m \times n}\)</span> be the feature mapping matrices of the two graphs, that is the matrices whose columns contain the vector representations of the vertices of the two graphs.
Then, the feature space Laplacian graph kernel is defined as</p>
<div class="math notranslate nohighlight">
\[k_{FLG}(G_1, G_2) = \frac{| (\frac{1}{2} \mathbf{S}_1^{-1} + \frac{1}{2} \mathbf{S}_2^{-1} )^{-1} |^{1/2}}{|\mathbf{S}_1|^{1/4} |\mathbf{S}_2|^{1/4}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{S}_1 = \mathbf{U}_1 \mathbf{L}_1^{-1} \mathbf{U}_1^\top + \gamma \mathbf{I}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{S}_2 = \mathbf{U}_2 \mathbf{L}_2^{-1} \mathbf{U}_2^\top + \gamma \mathbf{I}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the <span class="math notranslate nohighlight">\(m \times m\)</span> identity matrix.
Since the vertex features are local and invariant to vertex reordering, the feature space Laplacian graph kernel is permutation invariant.
Furthermore, since the distributions now live in the space of features rather than the space of vertices, the feature space Laplacian graph kernel can be applied to graphs of different sizes.</p>
<p>Let <span class="math notranslate nohighlight">\(\phi(v)\)</span> be the representation of vertex <span class="math notranslate nohighlight">\(v\)</span> constructed from local vertex features as described above.
The base kernel <span class="math notranslate nohighlight">\(\kappa\)</span> between two vertices <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span> corresponds to the dot product of their feature vectors</p>
<div class="math notranslate nohighlight">
\[\kappa(v_1, v_2) = \phi(v_1)^\top \phi(v_2)\]</div>
<p>Let <span class="math notranslate nohighlight">\(G_1\)</span> and <span class="math notranslate nohighlight">\(G_2\)</span> be two graphs with vertex sets <span class="math notranslate nohighlight">\(V_1 = \{ v_1, \ldots, v_{n_1}\}\)</span> and <span class="math notranslate nohighlight">\(V_2 = \{ u_1, \ldots, u_{n_2} \}\)</span> respectively, and let <span class="math notranslate nohighlight">\(\bar{V} = \{ \bar{v}_1, \ldots, \bar{v}_{n_1+n_2} \}\)</span> be the union of the two vertex sets.
Let also <span class="math notranslate nohighlight">\(\mathbf{K} \in \mathbb{R}^{(n_1+n_2) \times (n_1+n_2)}\)</span> be the kernel matrix defined as</p>
<div class="math notranslate nohighlight">
\[\mathbf{K}_{ij} = \kappa(\bar{v}_i, \bar{v}_j) = \phi(\bar{v}_i)^\top \phi(\bar{v}_j)\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{u}_1, \ldots, \mathbf{u}_p\)</span> be a maximal orthonormal set of the non-zero eigenvalue eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>
with corresponding eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \ldots, \lambda_p\)</span>.
Then the vectors</p>
<div class="math notranslate nohighlight">
\[\xi_i = \frac{1}{\sqrt{\lambda_i}} \sum_{l=1}^{n_1+n_2} [\mathbf{u}_i]_l \phi(\bar{v}_l)\]</div>
<p>where <span class="math notranslate nohighlight">\([\mathbf{u}_i]_l\)</span> is the <span class="math notranslate nohighlight">\(l^{th}\)</span> component of vector <span class="math notranslate nohighlight">\(\mathbf{u}_i\)</span> form an orthonormal basis for the subspace <span class="math notranslate nohighlight">\(\{ \phi(\bar{v}_1), \ldots, \phi(\bar{v}_{n_1+n_2}) \}\)</span>.
Moreover, let <span class="math notranslate nohighlight">\(\mathbf{Q} = [ \lambda_1^{1/2} \mathbf{u}_1, \ldots,\lambda_p^{1/2} \mathbf{u}_p ] \in \mathbb{R}^{p \times p}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Q}_1, \mathbf{Q}_2\)</span> denote the first <span class="math notranslate nohighlight">\(n_1\)</span> and last <span class="math notranslate nohighlight">\(n_2 \)</span>mathbf{Q}` respectively.
Then, the generalized feature space Laplacian graph kernel induced from the base kernel <span class="math notranslate nohighlight">\(\kappa\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[k_{FLG}^\kappa(G_1, G_2) = \frac{| (\frac{1}{2} \mathbf{S}_1^{-1} + \frac{1}{2} \mathbf{S}_2^{-1} )^{-1} |^{1/2}}{|\mathbf{S}_1|^{1/4} |\mathbf{S}_2|^{1/4}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{S}_1 = \mathbf{Q}_1 \mathbf{L}_1^{-1} \mathbf{Q}_1^\top + \gamma \mathbf{I}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{S}_2 = \mathbf{Q}_2 \mathbf{L}_2^{-1} \mathbf{Q}_2^\top + \gamma \mathbf{I}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the <span class="math notranslate nohighlight">\(p \times p\)</span> identity matrix.</p>
<p>The multiscale Laplacian graph kernel builds a hierarchy of nested subgraphs, where each subgraph is centered around a vertex and computes the generalized feature space Laplacian graph kernel between every pair of these subgraphs.
Let <span class="math notranslate nohighlight">\(G\)</span> be a graph with vertex set <span class="math notranslate nohighlight">\(V\)</span>, and <span class="math notranslate nohighlight">\(\kappa\)</span> a positive semi-definite kernel on <span class="math notranslate nohighlight">\(V\)</span>.
Assume that for each <span class="math notranslate nohighlight">\(v \in V\)</span>, we have a nested sequence of <span class="math notranslate nohighlight">\(L\)</span> neighborhoods</p>
<div class="math notranslate nohighlight">
\[v \in N_1(v) \subseteq N_2(v) \subseteq \ldots \subseteq N_L(v)\]</div>
<p>and for each <span class="math notranslate nohighlight">\(N_l(v)\)</span>, let <span class="math notranslate nohighlight">\(G_l(v)\)</span> be the corresponding induced subgraph of <span class="math notranslate nohighlight">\(G\)</span>.
The multiscale Laplacian subgraph kernels are defined as <span class="math notranslate nohighlight">\(\mathfrak{K}_1, \ldots, \mathfrak{K}_L : V \times V \rightarrow \mathbb{R}\)</span> as follows</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathfrak{K}_1\)</span> is just the generalized feature space Laplacian graph kernel <span class="math notranslate nohighlight">\(k_{FLG}^\kappa\)</span> induced from the base kernel <span class="math notranslate nohighlight">\(\kappa\)</span> between the lowest level subgraphs (ie the vertices)</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathfrak{K}_1(v,u) = k_{FLG}^\kappa(v, u)\]</div>
<ol class="arabic simple" start="2">
<li><p>For <span class="math notranslate nohighlight">\(l=2,3,\ldots,L\)</span>, <span class="math notranslate nohighlight">\(\mathfrak{K}_l\)</span> is the the generalized feature space Laplacian graph kernel induced from <span class="math notranslate nohighlight">\(\mathfrak{K}_{l-1}\)</span> between <span class="math notranslate nohighlight">\(G_l(v)\)</span> and <span class="math notranslate nohighlight">\(G_l(u)\)</span></p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathfrak{K}_l(v,u) = k_{FLG}^{\mathfrak{K}_{l-1}}(G_l(v), G_l(u))\]</div>
<p>Then, the multiscale Laplacian graph Kernel between two graphs <span class="math notranslate nohighlight">\(G_1, G_2\)</span> is defined as follows</p>
<div class="math notranslate nohighlight">
\[k_{MLG}(G_1, G_2) = k_{FLG}^{\mathfrak{K}_L}(G_1, G_2)\]</div>
<p>The multiscale Laplacian graph kernel computes <span class="math notranslate nohighlight">\(\mathfrak{K}_1\)</span> for all pairs of vertices, then computes <span class="math notranslate nohighlight">\(\mathfrak{K}_2\)</span> for all pairs of vertices, and so on.
Hence, it requires <span class="math notranslate nohighlight">\(\mathcal{O}(Ln^2)\)</span> kernel evaluations.
At the top levels of the hierarchy each subgraph centered around a vertex <span class="math notranslate nohighlight">\(G_l(v)\)</span> may have as many as <span class="math notranslate nohighlight">\(n\)</span> vertices.
Therefore, the cost of a single evaluation of the generalized feature space Laplacian graph kernel may take <span class="math notranslate nohighlight">\(\mathcal{O}(n^3)\)</span> time.
This means that in the worst case, the overall cost of computing <span class="math notranslate nohighlight">\(k_{MLG}\)</span> is <span class="math notranslate nohighlight">\(\mathcal{O}(Ln^5)\)</span>.
Given a dataset of <span class="math notranslate nohighlight">\(N\)</span> graphs, computing the kernel matrix requires repeating this for all pairs of graphs, which takes <span class="math notranslate nohighlight">\(\mathcal{O}(LN^2n^5)\)</span> time and is clearly problematic for real-world settings.</p>
<p>The solution to this issue is to compute for each level <span class="math notranslate nohighlight">\(l=1,2,\ldots,L+1\)</span> a single joint basis for all subgraphs at the given level across all graphs.
Let <span class="math notranslate nohighlight">\(G_1, G_2, \ldots, G_N\)</span> be a collection of graphs, <span class="math notranslate nohighlight">\(V_1, V_2, \ldots, V_N\)</span> their vertex sets, and assume that <span class="math notranslate nohighlight">\(V_1, V_2, \ldots, V_N \subseteq \mathcal{V}\)</span> for some general vertex space <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.
The joint vertex feature space of the whole graph collection is <span class="math notranslate nohighlight">\(W = span \big\{ \bigcup_{i=1}^N \bigcup_{v \in V_i} \{ \phi(v) \} \big\}\)</span>.
Let <span class="math notranslate nohighlight">\(c = \sum_{i=1}^N |V_i|\)</span> be the total number of vertices and <span class="math notranslate nohighlight">\(\bar{V} = (\bar{v}_1, \ldots, \bar{v}_c)\)</span> be the concatenation of the vertex sets of all graphs.
Let <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> be the corresponding joint kernel matrix and <span class="math notranslate nohighlight">\(\mathbf{u}_1, \ldots, \mathbf{u}_p\)</span> be a maximal orthonormal set of non-zero eigenvalue eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> with corresponding eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_p\)</span> and <span class="math notranslate nohighlight">\(p=dim(W)\)</span>.
Then the vectors</p>
<div class="math notranslate nohighlight">
\[\xi_i = \frac{1}{\sqrt{\lambda_i}} \sum_{l=1}^c [\mathbf{u}_i]_l \phi(\bar{v}_l) \qquad i=1,\ldots,p\]</div>
<p>form an orthonormal basis for <span class="math notranslate nohighlight">\(W\)</span>.
Moreover, let <span class="math notranslate nohighlight">\(\mathbf{Q} = [ \lambda_1^{1/2} \mathbf{u}_1, \ldots, \lambda_p^{1/2} \mathbf{u}_p ] \in \mathbb{R}^{p \times p}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Q}_1\)</span> denote the first <span class="math notranslate nohighlight">\(n_1\)</span> rows of matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{Q}_2\)</span> denote the next <span class="math notranslate nohighlight">\(n_2 \)</span>mathbf{Q}` and so on.
For any pair of graphs <span class="math notranslate nohighlight">\(G_i, G_j\)</span> of the collection, the generalized feature space Laplacian graph kernel induced from <span class="math notranslate nohighlight">\(\kappa\)</span> can be expressed as</p>
<div class="math notranslate nohighlight">
\[k_{FLG}^\kappa(G_i, G_j) = \frac{| (\frac{1}{2} \bar{\mathbf{S}}_i^{-1} + \frac{1}{2} \bar{\mathbf{S}}_j^{-1} )^{-1} |^{1/2}}{|\bar{\mathbf{S}}_i|^{1/4} |\bar{\mathbf{S}}_j|^{1/4}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\mathbf{S}}_i = \mathbf{Q}_i \mathbf{L}_i^{-1} \mathbf{Q}_i^\top + \gamma \mathbf{I}\)</span>, <span class="math notranslate nohighlight">\(\bar{\mathbf{S}}_j = \mathbf{Q}_j \mathbf{L}_j^{-1} \mathbf{Q}_j^\top + \gamma \mathbf{I}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the <span class="math notranslate nohighlight">\(p \times p\)</span> identity matrix.</p>
<p>The implementation of the multiscale Laplacian kernel can be found below</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="../generated/grakel.MultiscaleLaplacian.html#grakel.MultiscaleLaplacian" title="grakel.MultiscaleLaplacian"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiscaleLaplacian</span></code></a>([n_jobs,Â normalize,Â â¦])</p></td>
<td><p>Laplacian Graph Kernel as proposed in <a class="bibtex reference internal" href="#kondor2016multiscale" id="id3">[KP16]</a>.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="low-rank-approximation">
<h2>Low Rank Approximation<a class="headerlink" href="#low-rank-approximation" title="Permalink to this headline">Â¶</a></h2>
<p>Computing the kernel matrix between all vertices of all graphs (<span class="math notranslate nohighlight">\(c\)</span> vertices in total) and storing it is a very costly procedure.
Computing its eigendecomposition is even worse in terms of the required runtime.
Morever, <span class="math notranslate nohighlight">\(p\)</span> is also very large.
Managing the <span class="math notranslate nohighlight">\(\bar{\mathbf{S}}_1, \ldots, \bar{\mathbf{S}}_N\)</span> matrices (each of which is of size <span class="math notranslate nohighlight">\(p \times p\)</span>) becomes infeasible.
Hence, the multiscale Laplacian graph kernel replaces <span class="math notranslate nohighlight">\(W\)</span> with a smaller, approximate joint features space.
Let <span class="math notranslate nohighlight">\(\tilde{V} = (\tilde{v}_1, \ldots, \tilde{v}_{\tilde{c}})\)</span> be <span class="math notranslate nohighlight">\(\tilde{c} \ll c\)</span> vertices sampled from the joint vertex set.
Then, the corresponding subsampled vertex feature space is <span class="math notranslate nohighlight">\(\tilde{W} = span \{ \phi(v) : v \in \tilde{V} \}\)</span>.
Let <span class="math notranslate nohighlight">\(\tilde{p} = dim(\tilde{W})\)</span>.
Similarly to before, the kernel constructs an orthonormal basis <span class="math notranslate nohighlight">\(\{ \xi_1, \ldots, \xi_{\tilde{p}} \}\)</span> for <span class="math notranslate nohighlight">\(\tilde{W}\)</span> by forming the (now much smaller) kernel matrix <span class="math notranslate nohighlight">\(\mathbf{K}_{ij} = \kappa(\tilde{v}_i, \tilde{v}_j)\)</span>, computing its eigenvalues and eigenvectors, and setting <span class="math notranslate nohighlight">\(\xi_i = \frac{1}{\sqrt{\lambda_i}} \sum_{l=1}^{\tilde{c}} [\mathbf{u}_i]_l \phi(\tilde{v}_l)\)</span>.
The resulting approximate generalized feature space Laplacian graph kernel is</p>
<div class="math notranslate nohighlight">
\[k_{FLG}^\kappa(G_1, G_2) = \frac{| (\frac{1}{2} \tilde{\mathbf{S}}_1^{-1} + \frac{1}{2} \tilde{\mathbf{S}}_2^{-1} )^{-1} |^{1/2}}{|\tilde{\mathbf{S}}_1|^{1/4} |\tilde{\mathbf{S}}_2|^{1/4}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{\mathbf{S}}_1 = \tilde{\mathbf{Q}}_1 \mathbf{L}_1^{-1} \tilde{\mathbf{Q}}_1^\top + \gamma \mathbf{I}\)</span>, <span class="math notranslate nohighlight">\(\tilde{\mathbf{S}}_2 = \tilde{\mathbf{Q}}_2 \mathbf{L}_2^{-1} \tilde{\mathbf{Q}}_2^\top + \gamma \mathbf{I}\)</span> are the projections of <span class="math notranslate nohighlight">\(\bar{\mathbf{S}}_1\)</span> and <span class="math notranslate nohighlight">\(\bar{\mathbf{S}}_2\)</span> to <span class="math notranslate nohighlight">\(\tilde{W}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the <span class="math notranslate nohighlight">\(\tilde{p} \times \tilde{p}\)</span> identity matrix. Finally, the kernel introduces a further layer of approximation by restricting <span class="math notranslate nohighlight">\(\tilde{W}\)</span> to be the space spanned by the first <span class="math notranslate nohighlight">\(\hat{p} &lt; \tilde{p}\)</span> basis vectors (ordered by descending eigenvalue), effectively doing kernel PCA on <span class="math notranslate nohighlight">\(\{ \phi(\tilde{v}) \}_{\tilde{v} \in \tilde{V}}\)</span>.
The combination of these two factors makes computing the entire stack of kernels feasible, reducing the complexity of computing the kernel matrix for a dataset of <span class="math notranslate nohighlight">\(N\)</span> graphs to <span class="math notranslate nohighlight">\(\mathcal{O}(NL \tilde{c}^2 \hat{p}^3 + NL \tilde{c}^3 + N^2 \hat{p}^3)\)</span>.</p>
<p>The approximate multiscale Laplacian graph kernel can be found below</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
</tbody>
</table>
</div>
<div class="section" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">Â¶</a></h2>
<p id="bibtex-bibliography-kernels/multiscale_laplacian-0"><dl class="citation">
<dt class="bibtex label" id="kondor2016multiscale"><span class="brackets">KP16</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Risi Kondor and Horace Pan. The Multiscale Laplacian Graph Kernel. In <em>Advances in Neural Information Processing Systems</em>, 2990â2998. 2016.</p>
</dd>
<dt class="bibtex label" id="williams2001using"><span class="brackets"><a class="fn-backref" href="#id2">WS01</a></span></dt>
<dd><p>ChristopherÂ KI Williams and Matthias Seeger. Using the NystrÃ¶m Method to Speed Up Kernel Machines. In <em>Advances in Neural Information Processing Systems</em>, 682â688. 2001.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="neighborhood_hash.html" class="btn btn-neutral float-right" title="Neighborhood Hash Kernel" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="lovasz_theta.html" class="btn btn-neutral float-left" title="Lovasz Theta Kernel" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2019, DaSciM (BSD License).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>